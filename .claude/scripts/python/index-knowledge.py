#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script: index-knowledge.py
Description: Indexa conhecimento em .claude/memory/ em ChromaDB com embeddings locais
             Otimizado para Mac M3 (Apple Silicon)
Usage: python3 index-knowledge.py [--reindex]
Author: Claude + Anderson
Created: 2025-11-18
"""

import chromadb
from sentence_transformers import SentenceTransformer
import os
import glob
import sys
from pathlib import Path
from datetime import datetime
import hashlib

# ====== CONFIGURAÃ‡ÃƒO ======
MEMORY_PATH = "./.claude/memory/**/*.md"
VECTORDB_PATH = "./.claude/vectordb"
MODEL_NAME = 'all-MiniLM-L6-v2'  # 384 dimensÃµes, multilÃ­ngue

# Mac M3 optimizations
BATCH_SIZE = 64  # M3 tem memÃ³ria rÃ¡pida - processar mais de uma vez
NUM_THREADS = 8  # M3 tem 8 cores de performance

# ====== FUNÃ‡Ã•ES AUXILIARES ======

def get_file_hash(file_path):
    """Gera hash MD5 do arquivo para detectar mudanÃ§as"""
    with open(file_path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

def chunk_by_sections(content, file_path):
    """
    Divide markdown em seÃ§Ãµes (por ## headers)
    EstratÃ©gia: Cada seÃ§Ã£o = 1 chunk (melhor para RAG)
    """
    chunks = []
    lines = content.split('\n')

    current_section = ""
    current_header = "IntroduÃ§Ã£o"
    section_number = 0

    for line in lines:
        if line.startswith('## '):  # Header nÃ­vel 2
            # Salvar seÃ§Ã£o anterior
            if current_section.strip():
                chunks.append({
                    'content': current_section.strip(),
                    'header': current_header,
                    'section_number': section_number,
                    'file': file_path,
                    'char_count': len(current_section)
                })
                section_number += 1

            # Nova seÃ§Ã£o
            current_header = line.replace('## ', '').strip()
            current_section = line + '\n'

        elif line.startswith('### '):  # Subheader nÃ­vel 3 (incluir no chunk)
            current_section += line + '\n'

        else:
            current_section += line + '\n'

    # Ãšltima seÃ§Ã£o
    if current_section.strip():
        chunks.append({
            'content': current_section.strip(),
            'header': current_header,
            'section_number': section_number,
            'file': file_path,
            'char_count': len(current_section)
        })

    return chunks

def extract_tags(content):
    """Extrai tags do formato #tag1 #tag2"""
    import re
    tags = re.findall(r'#(\w+)', content)
    return list(set(tags))  # Unique tags

# ====== INICIALIZAÃ‡ÃƒO ======

print("ğŸš€ Iniciando indexaÃ§Ã£o RAG - Otimizado para Mac M3")
print(f"ğŸ“… Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Verificar se Ã© reindexaÃ§Ã£o
reindex = '--reindex' in sys.argv

if reindex:
    print("ğŸ”„ Modo REINDEXAÃ‡ÃƒO - apagando vector database anterior")
    import shutil
    if os.path.exists(VECTORDB_PATH):
        shutil.rmtree(VECTORDB_PATH)
    print("âœ… Database anterior removida\n")

print("ğŸ”§ Inicializando ChromaDB e modelo de embeddings...")

# ChromaDB
client = chromadb.PersistentClient(path=VECTORDB_PATH)
collection = client.get_or_create_collection(
    name="project_knowledge",
    metadata={
        "description": "Conhecimento do projeto Odoo 15 - RAG Vector Database",
        "created": datetime.now().isoformat(),
        "model": MODEL_NAME
    }
)

# Modelo de embeddings (otimizado para M3)
print(f"ğŸ“¦ Carregando modelo: {MODEL_NAME}")
print("âš¡ Detectando aceleraÃ§Ã£o de hardware...")

# Mac M3 usa MPS (Metal Performance Shaders)
import torch
if torch.backends.mps.is_available():
    device = "mps"  # Apple Silicon GPU
    print("âœ… GPU Apple M3 detectada - usando MPS (Metal)")
elif torch.cuda.is_available():
    device = "cuda"
    print("âœ… CUDA detectada")
else:
    device = "cpu"
    print("âš ï¸  Usando CPU (sem aceleraÃ§Ã£o)")

model = SentenceTransformer(MODEL_NAME, device=device)
print(f"âœ… Modelo carregado em: {device}")
print(f"âœ… ChromaDB em: {VECTORDB_PATH}\n")

# ====== INDEXAÃ‡ÃƒO ======

print("ğŸ“š Escaneando arquivos...")
files = sorted(glob.glob(MEMORY_PATH, recursive=True))
print(f"ğŸ“Š Encontrados: {len(files)} arquivos\n")

total_chunks = 0
total_files = 0
total_chars = 0
skipped_files = 0

for file_path in files:
    try:
        file_name = Path(file_path).name

        # Ler arquivo
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Hash para detectar mudanÃ§as
        file_hash = get_file_hash(file_path)

        # Verificar se jÃ¡ indexado (skip se nÃ£o mudou)
        existing_docs = collection.get(where={"file_path": file_path})
        if existing_docs['ids'] and not reindex:
            # Verificar hash
            old_hash = existing_docs['metadatas'][0].get('file_hash', '')
            if old_hash == file_hash:
                skipped_files += 1
                print(f"  â­ï¸  {file_name}: nÃ£o modificado, pulando")
                continue
            else:
                # Arquivo mudou - deletar chunks antigos
                collection.delete(where={"file_path": file_path})
                print(f"  ğŸ”„ {file_name}: arquivo modificado, reindexando")

        # Chunking
        chunks = chunk_by_sections(content, file_path)

        if not chunks:
            print(f"  âš ï¸  {file_name}: sem conteÃºdo, pulando")
            continue

        # Gerar embeddings em batch (M3 optimization)
        print(f"  ğŸ”„ {file_name}: gerando embeddings para {len(chunks)} chunks...")
        texts = [chunk['content'] for chunk in chunks]

        # Batch encoding (mais rÃ¡pido no M3)
        embeddings = model.encode(
            texts,
            batch_size=BATCH_SIZE,
            show_progress_bar=False,
            convert_to_numpy=True
        ).tolist()

        # Adicionar ao ChromaDB
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            doc_id = f"{Path(file_path).stem}_{i}_{file_hash[:8]}"

            # Extrair tags
            tags = extract_tags(chunk['content'])

            collection.add(
                embeddings=[embedding],
                documents=[chunk['content']],
                metadatas=[{
                    'file_path': file_path,
                    'file_name': file_name,
                    'header': chunk['header'],
                    'section_number': chunk['section_number'],
                    'char_count': chunk['char_count'],
                    'file_hash': file_hash,
                    'indexed_at': datetime.now().isoformat(),
                    'tags': ','.join(tags) if tags else ''
                }],
                ids=[doc_id]
            )

        total_chunks += len(chunks)
        total_chars += sum(c['char_count'] for c in chunks)
        total_files += 1
        print(f"  âœ… {file_name}: {len(chunks)} chunks indexados")

    except Exception as e:
        print(f"  âŒ Erro em {file_path}: {e}")

# ====== ESTATÃSTICAS ======

print(f"\n{'='*60}")
print("âœ… INDEXAÃ‡ÃƒO COMPLETA!")
print(f"{'='*60}")
print(f"ğŸ“Š Arquivos processados: {total_files}")
print(f"ğŸ“Š Arquivos pulados (nÃ£o modificados): {skipped_files}")
print(f"ğŸ“Š Total de chunks: {total_chunks}")
print(f"ğŸ“Š Total de caracteres: {total_chars:,}")
print(f"ğŸ“Š MÃ©dia chars/chunk: {total_chars//total_chunks if total_chunks > 0 else 0}")
print(f"ğŸ“‚ Vector database: {VECTORDB_PATH}")
print(f"ğŸ’¾ Tamanho database: {sum(f.stat().st_size for f in Path(VECTORDB_PATH).rglob('*') if f.is_file()) / 1024 / 1024:.2f} MB")
print(f"âš¡ Device usado: {device.upper()}")

# ====== TESTE RÃPIDO ======

print(f"\n{'='*60}")
print("ğŸ§ª TESTE DE BUSCA")
print(f"{'='*60}")

test_queries = [
    "Como resolver erro de rede no Odoo?",
    "Comandos para reiniciar Odoo",
    "Patterns de performance Python"
]

for test_query in test_queries:
    print(f"\nğŸ“ Pergunta: '{test_query}'")

    # Embedding da query
    query_embedding = model.encode(test_query).tolist()

    # Buscar
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3
    )

    if results['documents'][0]:
        print("   Top 3 resultados:")
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
            print(f"   {i}. {metadata['file_name']} - {metadata['header']}")
            preview = doc[:80].replace('\n', ' ')
            print(f"      {preview}...")
    else:
        print("   âš ï¸  Nenhum resultado encontrado")

print(f"\n{'='*60}")
print("ğŸ‰ RAG Vector Database pronto para uso!")
print(f"{'='*60}\n")
